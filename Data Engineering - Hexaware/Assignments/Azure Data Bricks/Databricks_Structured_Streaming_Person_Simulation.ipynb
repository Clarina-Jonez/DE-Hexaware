{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "272f5617-1d21-4e68-ac28-9527af9eecfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1️⃣ Prep: Create folders & split your Person.json into smaller files\n",
    "This splits your uploaded /FileStore/tables/Person.json into individual JSON files so we can “feed” them into the stream one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f6fa18-e199-49fa-ba83-2d4d5fdb84c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 5 split files in: /dbfs/FileStore/streaming_parts/\nNext: We will 'release' these into the input folder while stream runs.\n"
     ]
    }
   ],
   "source": [
    "import json, os, shutil, glob\n",
    "\n",
    "# Paths\n",
    "uploaded = \"/dbfs/FileStore/tables/Person.json\"       # already uploaded file\n",
    "parts_dir = \"/dbfs/FileStore/streaming_parts/\"        # staging: all split files\n",
    "input_dir = \"/dbfs/FileStore/streaming_input/person/\" # folder watched by stream\n",
    "\n",
    "# Clean old runs\n",
    "shutil.rmtree(parts_dir, ignore_errors=True)\n",
    "shutil.rmtree(input_dir, ignore_errors=True)\n",
    "os.makedirs(parts_dir, exist_ok=True)\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Read uploaded file (handle both array JSON & NDJSON)\n",
    "raw = open(uploaded, \"r\", encoding=\"utf-8\").read().strip()\n",
    "items = []\n",
    "try:\n",
    "    parsed = json.loads(raw)\n",
    "    if isinstance(parsed, list):\n",
    "        items = parsed\n",
    "    else:\n",
    "        items = [parsed]\n",
    "except Exception:\n",
    "    for line in raw.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                items.append(json.loads(line))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Write each record to its own file in staging folder\n",
    "for i, obj in enumerate(items, start=1):\n",
    "    fname = os.path.join(parts_dir, f\"person_part_{i:03d}.json\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "print(f\"✅ Created {len(items)} split files in: {parts_dir}\")\n",
    "print(\"Next: We will 'release' these into the input folder while stream runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a73a7a5f-834a-40a2-adc1-8c67739c280d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2️⃣ Helper: Release the next file into the stream input\n",
    "Run this cell each time you want to simulate a new event arriving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8059c51f-d1a4-4a6d-8cf7-1eede43f470e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No more files to release.\n"
     ]
    }
   ],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "parts = sorted(glob.glob(parts_dir + \"person_part_*.json\"))\n",
    "if not parts:\n",
    "    print(\"❌ No more files to release.\")\n",
    "else:\n",
    "    src = parts[0]\n",
    "    dest = os.path.join(input_dir, os.path.basename(src))\n",
    "    shutil.move(src, dest)\n",
    "    print(f\"\uD83D\uDCE4 Released {os.path.basename(src)} to stream input folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c90758c8-db6d-414c-ab79-653e3786cd09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3️⃣ Define the schema for the Person records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5929f420-2718-406e-95c5-670bb4b8dcea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "person_schema = (StructType()\n",
    "    .add(\"id\", IntegerType())\n",
    "    .add(\"firstname\", StringType())\n",
    "    .add(\"middlename\", StringType())\n",
    "    .add(\"lastname\", StringType())\n",
    "    .add(\"dob_year\", IntegerType())\n",
    "    .add(\"dob_month\", StringType())  # month might be string or int; adjust if needed\n",
    "    .add(\"gender\", StringType())\n",
    "    .add(\"salary\", IntegerType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbab9395-7fce-4f81-a9f6-98beaeb1f7d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4️⃣ Create the streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b08edc-9498-4e58-9354-7017f7d6d717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob_year: integer (nullable = true)\n |-- dob_month: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/FileStore/streaming_input/person/\"\n",
    "\n",
    "person_stream = (spark.readStream\n",
    "                 .schema(person_schema)\n",
    "                 .json(input_path))\n",
    "\n",
    "person_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b319cb4c-4105-4f6c-91bf-210e8b7a1aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5️⃣ Start console output stream\n",
    "Keep this running in one cell — then switch back to cell 2 to “release” files. You should see each record appear as you release it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92d234a-7229-4502-96de-3300ede1d877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_console = (person_stream.writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"append\")\n",
    "                 .option(\"truncate\", False)\n",
    "                 .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6a2574-08da-40e2-8ce9-810f6faffa4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####6️⃣ (Optional) Persist to Delta\n",
    "If you also want to save processed records to Delta for later queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52c4f5e-4667-48db-a623-02b67f738406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_path = \"/dbfs/tmp/ss_tutorial/target_delta/\"\n",
    "checkpoint_path = \"/dbfs/tmp/ss_tutorial/checkpoint_delta/\"\n",
    "\n",
    "shutil.rmtree(target_path, ignore_errors=True)\n",
    "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "delta_query = (person_stream.writeStream\n",
    "               .format(\"delta\")\n",
    "               .option(\"path\", target_path)\n",
    "               .option(\"checkpointLocation\", checkpoint_path)\n",
    "               .outputMode(\"append\")\n",
    "               .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "045d8f60-847a-4744-adc9-56a5f837bb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Later, read saved data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9058d00e-bdd8-4635-8f06-9bd6244b142f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+--------+---------+------+------+\n| id|firstname|middlename|lastname|dob_year|dob_month|gender|salary|\n+---+---------+----------+--------+--------+---------+------+------+\n|  3|  Robert |          |Williams|    2010|        3|     M|  4000|\n|  4|   Maria |      Anne|   Jones|    2005|        5|     F|  4000|\n|  2| Michael |      Rose|        |    2010|        3|     M|  4000|\n|  1|   James |          |   Smith|    2018|        1|     M|  3000|\n|  5|      Jen|      Mary|   Brown|    2010|        7|      |    -1|\n+---+---------+----------+--------+--------+---------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(target_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64cb9f9e-7a4f-4c9e-a32e-37ac88e14003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####7️⃣ Stop the streams when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c986890-c62e-4b5c-9e82-af2431b7a229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All active streams stopped.\n"
     ]
    }
   ],
   "source": [
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "print(\"✅ All active streams stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b3be01-bdb2-41b8-a9c7-6b9a36e9bd98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####\uD83D\uDD39 How to use it in real-time:\n",
    "\n",
    "- Run cell 1 — split your big JSON file into small chunks.\n",
    "\n",
    "- Run cell 4 — start the stream by creating the DataFrame.\n",
    "\n",
    "- Run cell 5 — start the console output.\n",
    "\n",
    "- Run cell 2 repeatedly — each time you run it, one more JSON file is dropped into the input folder, and you’ll see a new record appear in the console output in real-time.\n",
    "\n",
    "- When done, run cell 7 to stop the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461130d6-7e57-4efd-b507-5498088b5669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "____\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_Structured_Streaming_Person_Simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}