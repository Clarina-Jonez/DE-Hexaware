{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e6673a-fa58-4534-be3f-34b525184768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###PART A — File-based streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42523031-6374-4595-b84c-12b5c02490c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Goal: simulate streaming using your Person.json by splitting it into smaller files and releasing them one-by-one into an input folder. We'll then show console output and write to Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b27f3e78-61c8-4d6d-825a-d89828d1c73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A0 — Create helper file chunks \n",
    "\n",
    "- This cell reads your uploaded /FileStore/tables/Person.json and writes each JSON object into its own file in a staging folder.\n",
    "\n",
    "- You will release files from staging into the live input folder later (one at a time), which simulates new incoming files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea1ddb7-251a-493d-a61e-6ff7c681b078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, os, glob\n",
    "\n",
    "# Paths (driver/local view)\n",
    "uploaded = \"/dbfs/FileStore/tables/Person.json\"    # your uploaded file\n",
    "parts_dir = \"/dbfs/FileStore/streaming_parts/\"     # staging area (all parts go here)\n",
    "input_dir = \"/dbfs/FileStore/streaming_input/person/\"  # where the stream will read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf5e0dc-2138-48e3-9c08-7347e83699bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5 part files to /dbfs/FileStore/streaming_parts/\nTo simulate streaming, we will move these files one-by-one into the input folder during the exercise.\n"
     ]
    }
   ],
   "source": [
    "# Create directories (if they already exist this will overwrite files inside)\n",
    "import shutil\n",
    "shutil.rmtree(parts_dir, ignore_errors=True)\n",
    "shutil.rmtree(input_dir, ignore_errors=True)\n",
    "os.makedirs(parts_dir, exist_ok=True)\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Read uploaded file robustly: handle newline-delimited JSON or an array of objects\n",
    "raw = open(uploaded, \"r\", encoding=\"utf-8\").read().strip()\n",
    "items = []\n",
    "try:\n",
    "    # try a json array\n",
    "    parsed = json.loads(raw)\n",
    "    if isinstance(parsed, list):\n",
    "        items = parsed\n",
    "    else:\n",
    "        items = [parsed]\n",
    "except Exception:\n",
    "    # fallback: try parsing line-by-line (ndjson)\n",
    "    for line in raw.splitlines():\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            items.append(json.loads(line))\n",
    "        except Exception:\n",
    "            # ignore badly formed lines\n",
    "            pass\n",
    "\n",
    "# Write each record as a separate file in the parts_dir\n",
    "for i, obj in enumerate(items, start=1):\n",
    "    fname = os.path.join(parts_dir, f\"person_part_{i:03d}.json\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)\n",
    "print(f\"Wrote {len(items)} part files to {parts_dir}\")\n",
    "print(\"To simulate streaming, we will move these files one-by-one into the input folder during the exercise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9af615-f07c-45a9-8dc6-09b4ec900185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A1 — Function to release the next file (manual simulation)\n",
    "- Each time you run this cell it will move one part file from streaming_parts into the streaming_input/person folder. Run it while the stream is running to simulate incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c28c19d-628a-4f3f-8372-82670e878b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more part files to release.\n"
     ]
    }
   ],
   "source": [
    "# Python cell: run this each time you want to push the next file into the stream\n",
    "import os, glob, shutil\n",
    "parts_dir = \"/dbfs/FileStore/streaming_parts/\"\n",
    "input_dir = \"/dbfs/FileStore/streaming_input/person/\"\n",
    "\n",
    "parts = sorted(glob.glob(parts_dir + \"person_part_*.json\"))\n",
    "if not parts:\n",
    "    print(\"No more part files to release.\")\n",
    "else:\n",
    "    src = parts[0]\n",
    "    dest = os.path.join(input_dir, os.path.basename(src))\n",
    "    shutil.move(src, dest)\n",
    "    print(\"Released:\", os.path.basename(src), \"→\", dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a137ca-84a9-448e-809d-db9abd9538d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A2 — Create the streaming DataFrame (file stream)\n",
    "This creates a streaming DataFrame that watches /FileStore/streaming_input/person/ and expects the Person schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57ae116-5b4d-43fa-9244-dceada570a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob_year: integer (nullable = true)\n |-- dob_month: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "person_schema = (StructType()\n",
    "    .add(\"id\", IntegerType())\n",
    "    .add(\"firstname\", StringType())\n",
    "    .add(\"middlename\", StringType())\n",
    "    .add(\"lastname\", StringType())\n",
    "    .add(\"dob_year\", IntegerType())\n",
    "    .add(\"dob_month\", IntegerType())\n",
    "    .add(\"gender\", StringType())\n",
    "    .add(\"salary\", IntegerType())\n",
    ")\n",
    "\n",
    "input_path = \"/FileStore/streaming_input/person/\"   # Spark will read this path\n",
    "person_stream = (spark.readStream\n",
    "                 .schema(person_schema)   # required for file streams\n",
    "                 .json(input_path)\n",
    "                )\n",
    "\n",
    "person_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93d127c-9c75-4aa4-8ddc-65625c1f68e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A3 — Start a console writer to watch incoming records\n",
    "Start this cell and keep it running while you release parts (run the \"release next file\" cell to see records appear):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f535935-0557-4761-9d8e-919b3a45de90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Console stream started. Release files (run release cell) to see output.\n"
     ]
    }
   ],
   "source": [
    "query_console = (person_stream.writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"append\")\n",
    "                 .option(\"truncate\", False)\n",
    "                 .start()\n",
    "                )\n",
    "print(\"Console stream started. Release files (run release cell) to see output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef36b0fd-63fe-4c9f-ac5e-4b900f8a2250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A4 — Persist stream to Delta (safe pattern with checkpoint)\n",
    "When you’re happy with testing, write the stream to Delta so you can query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8ca06f-81a4-40fe-b902-4d521328ec2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta write started. New files will land at: /dbfs/tmp/ss_tutorial/target_delta/\n"
     ]
    }
   ],
   "source": [
    "# Python cell (paste & run)\n",
    "target_path = \"/dbfs/tmp/ss_tutorial/target_delta/\"    # where Delta files are stored\n",
    "checkpoint = \"/dbfs/tmp/ss_tutorial/checkpoint_delta/\"  # unique checkpoint\n",
    "\n",
    "# Remove old test folders if needed (be careful!)\n",
    "import shutil\n",
    "shutil.rmtree(target_path, ignore_errors=True)\n",
    "shutil.rmtree(checkpoint, ignore_errors=True)\n",
    "\n",
    "delta_query = (person_stream.writeStream\n",
    "               .format(\"delta\")\n",
    "               .option(\"path\", target_path)\n",
    "               .option(\"checkpointLocation\", checkpoint)\n",
    "               .outputMode(\"append\")\n",
    "               .start()\n",
    "              )\n",
    "print(\"Delta write started. New files will land at:\", target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6c3e1c-6e4c-4393-8197-a82541ce3047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####A5 — Stop streams (VERY IMPORTANT)\n",
    "When you are done with the interactive console or writing streams, stop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa7d2b8-dc3a-48bb-bcba-47811a053426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping: f98892ff-e57a-43df-8201-0e05ad8b47bd None\nAll streams stopped.\n"
     ]
    }
   ],
   "source": [
    "for q in spark.streams.active:\n",
    "    print(\"Stopping:\", q.id, q.name)\n",
    "    q.stop()\n",
    "print(\"All streams stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf397452-3c12-441a-94e7-e2f6e3c42eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb77648-00d0-48bc-b18a-57e57327f230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_Structured_Streaming_Tutorial_and_Joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}